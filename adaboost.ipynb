{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Autoren: Alexander Walla, Lennard Langenbruch, Marc-Alexander Richts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "\n",
    "*Boosting* bezeichnet in Machine Learning einen \"[ensemble](https://de.wikipedia.org/wiki/Ensemble_learning) [Metaalgorithmus](https://de.wikipedia.org/wiki/Metaheuristik)\". Ensemble Learning verwendet mehrere Lernalgorithmen, um prädikative [Inferenz](https://de.wikipedia.org/wiki/Statistische_Inferenz) (Vorhersagen) im Vergleich zu einzelnen Lernalgorithmen zu verbessern. Boosting ist eine Art von Implementation von Ensemble Learning um mehrere schwache Klassifikatoren zu einem leistungsfähigen Klassifikator zusammenzufügen. Beim Boosting werden die Prädikatoren in \"Reihe geschaltet\" und nacheinander trainiert, sodass jeder Prädikator versucht die Fehler des vorherigen Prädikators zu beheben.\n",
    "\n",
    "Die einzelnen schwachen Klassifikatoren müssen nur leicht besser als zufälliges raten sein."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AdaBoost\n",
    "\n",
    "AdaBoost (kurz für \"Adaptive Boost\") ist eine Boostingmethode, die den vorherigen Prädikator korrigiert, indem der nachfolgende Prädikator seinen Bias (Gewichte) zu den im Vorgängerprädikator nicht beachteten Trainingsdatenpunkten schiebt. Damit fokussieren sich die in der Kette später kommenden Prädikatoren mehr und mehr auf die schwierigeren Fälle. [1]\n",
    "\n",
    "Je nach Genauigkeit der einzelnen Prädikatoren werden diese gewichtet. Je genauer ein Prädikator ist, desto höher ist sein Gewicht. Bei einer Genauigkeit, die zufälligem raten entspricht, bekommt der Prädikator ein Gewicht von ``0``. Wenn der Prädikator eine niedrigere Genauigkeit als 50% Trefferquote hat, bekommt er ein negatives Gewicht. [2]\n",
    "\n",
    "**Es gibt also zwei Arten von Gewichten**:\n",
    "- Die Trainingsdaten bekommen Gewichte $ 1 / m $. Zuerst sind diese Datenpunkte alle neutral $ m = 1 => 1 / 1 = 1 $ gewichtet, das bedeutet, der erste Prädikator wird auf die originalen Daten trainiert. Danach wird die gewichtete Fehlerquote auf die Trainingsdaten berechnet.\n",
    "- Alle Prädikatoren bekommen je nach Genauigkeit ein hohes oder niedriges Gewicht. Dieses Gewicht wird am Ende im Ensemble verwendet, um eine Mehrheit auszurechnen. Das Gewicht eines Prädikators $ a_{j} $ berechnet sich wie folgt:\n",
    "\n",
    "$$ a_{j} = n log \\frac{1 - r_{j}}{r_{j}} $$\n",
    "$$(Géron, 2020, p. 251)$$\n",
    "\n",
    "- Der Hyperparameter $ n $ stellt hier die Lernrate dar, der standardmäßig auf 1 gesetzt ist.\n",
    "- $ r_{j} $ beschreibt die Fehlerrate\n",
    "\n",
    "\n",
    "Danach werden die falsch klassifizierten Datenpunkte *geboosted*; diese Datenpunkte bekommen ein höheres Gewicht. Wie oben bereits beschrieben, verschiebt sich dadurch der Bias auf die Datenpunkte auf die schwierigeren Cases.\n",
    "\n",
    "\n",
    "## Vergleich von AdaBoost Prädikatoren zu Random Forests\n",
    "Der Vergleich von AdaBoost zu Random Forests eignet sich sehr gut, da viele Merkmale dieser Algorithmen invers sind.\n",
    "- Bei Random Forests hat jeder Tree (Prädikator) ein gleichwertiges Gewicht; bei AdaBoost bekommen die Prädikatoren ein verschieden schweres Gewicht wodurch einige Prädikatoren \"mehr zu sagen\" haben als andere.\n",
    "- Bei Random Forests können die Trees parallel auf einzelne Merkmale trainiert werden. Bei AdaBoost werden die Learner jedoch sequenziell trainiert, wodurch sich das training nur sehr schwierig skalieren lässt. Denn je nachdem wie genau der erste Learner ist, werden die Datenpunkte für den zweiten Learner geboosted. Diese Abhängigkeit führt zu einer langen Kaskade.\n",
    "\n",
    "## Applikation in der realen Welt\n",
    "Eine der interessantesten Anwendungen von AdaBoost ist schnelle und effiziente Objektklassifizierung auf (aus der heutigen Sicht) Computern mit niedriger Rechenleistung. 2001 haben Viola und Jones auf einem 700MHz Pentium III auf einem 384 x 288 Pixel-Foto mit 15 Frames pro Sekunde klassifizieren können, ob das Bild ein Gesicht enthält oder nicht.\n",
    "Bei Viola und Jones wird jeder schwache Lerner von AdaBoost auf ein einzelnes Merkmal trainiert. Dann wird der jeweilige Lerner mit der geringsten Fehlerrate genommen. Dadurch entsteht bereits im Trainingsschritt eine Art Selektion. Da Viola und Jones am Anfang der Klassifizierungskaskade weniger komplexe Learner verwenden, kann ein Gesicht im Klassifizierungsprozess bereits relativ früh abgelehnt werden, wodurch die späteren komplexeren Learner teilweise gar nicht erst aufgerufen werden - wodurch die Performance des Algorithmus im echten Leben gesteigert wird.\n",
    "\n",
    "*Anmerkung: Der Teil vom Paper von Viola und Jones [5], der sich auf AdaBoost bezieht, ist sehr simpel und verständlich geschrieben. Für alle, die sich mehr mit diesem Thema beschäftigen wollen, und sogar Machine Learning auf embedded Systemen anwenden möchten, ist ein Blick auf das Paper ab Kapitel 3 \"Learning Classification Functions\" sehr viel wert.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AdaBoost in SciKit-Learn\n",
    "\n",
    "SciKit-Learn implementiert eine angepasste Version von AdaBoost namens *Discrete SAMME*. SAMME steht für **S**tagewise **A**dditive **M**odeling using a **M**ulticlass **E**xponential loss function.\n",
    "Diese angepasste Version nach dem Paper \"Multi-class AdaBoost\" [3] kann im Gegensatz zum originalen AdaBoost nicht nur binäre Entscheidungen treffen, sondern in mehr als nur zwei Kategorien unterscheiden.\n",
    "\n",
    "Wenn man Prädikatoren verwendet, welche nicht nur binäre Entscheidungen, sondern Wahrscheinlichkeiten zurückgeben, kann man außerdem eine abgewandelte Form von SAMME namens *SAMME.R* verwenden, wobei das R für **R**eal steht.\n",
    "\n",
    "Der Unterschied zwischen SAMME und SAMME.R ist dass SAMME sich auf der Grundlage von Fehlern in den vorhergesagten Class Labels sich anpasst, während SAMME.R die vorhergesagten Klassenwahrscheinlichkeiten verwendet. SAMME.R erzielt im Allgemeinen oft bessere Ergebnisse als SAMME.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Praktische Anwendung anhand eines Beispiels\n",
    "\n",
    "Das folgende Beispiel bezieht sich auf die Erkennung von Kreditkartenbetrug (Fraud). Die Daten stammen von Kaggle [6]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('card_transdata.csv')\n",
    "dataset.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Übersicht verschaffen mit was für Daten wir überhaupt arbeiten.\n",
    "# Wie viele Transaktionen in unserem Dataset sind Fraud?\n",
    "non_fraud, fraud = dataset['fraud'].value_counts()\n",
    "print(non_fraud, fraud)\n",
    "fraud_ratio =  ( fraud / (non_fraud + fraud) ) * 100\n",
    "print(f'{fraud_ratio:.2f}% of {non_fraud + fraud} transactions were fraudulent.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wo traten die meisten Transaktionen mit Betrug auf?\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Source: https://www.kaggle.com/code/gerardocappa/card-fraud-detection-with-randomforest [7]\n",
    "for col in ['repeat_retailer','used_chip','used_pin_number','online_order']:\n",
    "    sns.countplot(data=dataset, x=col, hue='fraud')\n",
    "    plt.show()\n",
    "\n",
    "# In den folgenden Plots können wir sehen, dass Transaktionen, welche mit PIN getätigt wurden,\n",
    "# meistens keine betrügerischen Transaktionen sind.\n",
    "# Außerdem findet ein großer Teil der betrügerischen Transaktionen online statt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datenset kopieren\n",
    "orig_dataset = dataset.copy()\n",
    "features_names = dataset.columns.tolist()\n",
    "# Fraud von der Featurelist entfernen\n",
    "features_names.remove('fraud')\n",
    "\n",
    "# Features normalisieren\n",
    "minmax_scaler = MinMaxScaler()\n",
    "orig_dataset[features_names] = minmax_scaler.fit_transform(dataset[features_names])\n",
    "\n",
    "# Fraud-Column aus dem Datenset entfernen, und als testdaten speichern\n",
    "y_sc = orig_dataset.pop('fraud')\n",
    "# Split Dataset 80-20. train_test_split wie in den Praktika verwendet.\n",
    "X_train, X_test, y_train, y_test = train_test_split(orig_dataset,y_sc, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# AdaBoost Kaskade mit 100 DecisionTrees instanziieren\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=100, algorithm='SAMME.R',learning_rate=0.5)\n",
    "\n",
    "# Train model\n",
    "%timeit ada.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Genauigkeit messen\n",
    "ada.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wie man hier schön erkennen kann, ist unser Model sehr akkurat und wurde auf schwacher Hardware trainiert (Laptop von 2020 mit i7-8550U). Das ganze hat nur wenige Minuten gedauert.\n",
    "Das Anwenden des Models liegt im Millisekundenbereich. Es bietet sich daher vor allem für schwächere Hardware an, oder wo ein komplexeres Model einfach Overkill wäre."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hinweise zur Praxis\n",
    "\n",
    "- Die Lernrate von ``1`` ist laut Peter Prettenhofer und Noel Dawe [4] nicht unbedingt für SAMME und SAMME.R beidermaßen optimal. Am Besten spielt man mit dem Hyperparameter ein wenig herum, bis man seinem gewünschtem Ergebnis nahe kommt.\n",
    "- Wenn das Model zu Overfitting neigt, kann es helfen die Anzahl der Lerner ``n_estimators`` zu verringern."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Literaturverzeichnis\n",
    "[1] sklearn.ensemble.AdaBoostClassifier. (n.d.). Scikit-Learn. Retrieved June 3, 2022, from [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "[2] Géron, A. (2020). Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow: Konzepte, Tools und Techniken für intelligente Systeme (Aktuell zu TensorFlow 2) [E-book]. Dpunkt.Verlag GmbH.\n",
    "[3] J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”, 2009.\n",
    "[4] Dawe, N., & Prettenhofer, P. (n.d.). Discrete versus Real AdaBoost. Scikit-Learn. Retrieved June 3, 2022, from [https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html#preparing-the-data-and-baseline-models](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html#preparing-the-data-and-baseline-models)\n",
    "[5] Viola, P., & Jones, M. (2001). Rapid Object Detection using a Boosted Cascade of Simple Features. University of Texas. Retrieved June 14, 2022, from [https://www.cs.utexas.edu/~grauman/courses/spring2007/395T/papers/viola_cvpr2001.pdf](https://www.cs.utexas.edu/~grauman/courses/spring2007/395T/papers/viola_cvpr2001.pdf)\n",
    "[6] Narayanan, D. (2022, May 7). Credit Card Fraud. Kaggle. Retrieved June 17, 2022, from [https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud](https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud)\n",
    "[7] Cappa, G. (2022, June 12). Card Fraud Detection with RandomForest. Kaggle. https://www.kaggle.com/code/gerardocappa/card-fraud-detection-with-randomforest  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}